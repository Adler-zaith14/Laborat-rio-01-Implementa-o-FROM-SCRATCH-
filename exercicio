import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns
import math

class MecanismoAtencaoManual(nn.Module):
    """
    Laboratório AULA02: Implementação 'From Scratch' da formula Matricial ATTENTION(q,k,v) = softmax.
    Esta classe demonstra o domínio da álgebra linear por trás do mecanismo de Scaled Dot-Product Attention.
    """
    def __init__(self, d_modelo):
        super(MecanismoAtencaoManual, self).__init__()

        self.d_k = d_modelo

        # Requisito 2: Projeções lineares para gerar Query (Q), Key (K) e Value (V)
        # Camadas aprendíveis que mapeiam a entrada para subespaços de representação, visto no conteudo da aula 02 de pipeline do multi-head em diante dos conteudos
        self.w_query = nn.Linear(d_modelo, d_modelo)
        self.w_key = nn.Linear(d_modelo, d_modelo)
        self.w_value = nn.Linear(d_modelo, d_modelo)

    def forward(self, x):
        # Gerando as matrizes de projeção
        Q = self.w_query(x)
        K = self.w_key(x)
        V = self.w_value(x)

        # Requisito 3: Algoritmo Scaled implementado manualmente
        # Passo 1: Produto escalar entre a Query e a Key transposta (Alignment Scores)
        # O uso de .transpose(-2, -1) garante a compatibilidade para multiplicação matricial
        scores = torch.matmul(Q, K.transpose(-2, -1))

        # Passo 2: Escalonamento pela raiz quadrada de d_k
        # Essencial para estabilidade numérica, evitando que o softmax sature [5, 6]
        scores_escalonados = scores / math.sqrt(self.d_k)

        # Passo 3: Softmax para obter a distribuição de probabilidade (pesos de atenção)
        # A soma de cada linha resultará em 1.0 [7, 8]
        pesos_atencao = F.softmax(scores_escalonados, dim=-1)

        # Passo 4: Soma ponderada dos valores (saída rica em contexto)
        saida_contextualizada = torch.matmul(pesos_atencao, V)

        return saida_contextualizada, pesos_atencao



# Configurações de reprodutibilidade
torch.manual_seed(42)
seq_len = 8       
dim_modelo = 16   

# simulando um lote com uma frase (Batch=1, Seq=8, Emb=16)
entrada_x = torch.randn(1, seq_len, dim_modelo)

# Inicialização da camada attetion
atencao_lab = MecanismoAtencaoManual(d_modelo=dim_modelo)
saida, pesos = atencao_lab(entrada_x)

# Requisito pra Gerar Heatmap para análise visual dos pesos
def plotar_heatmap_atencao(matriz_pesos):
    plt.figure(figsize=(10, 8))

   
    pesos_2d = matriz_pesos[0].detach().cpu().numpy() 

    sns.heatmap(pesos_2d,
                annot=True,
                fmt=".2f",
                cmap="YlGnBu",
                xticklabels=[f"Key_{i}" for i in range(seq_len)],
                yticklabels=[f"Query_{i}" for i in range(seq_len)])

    plt.title("Visualização Contextual: Mapa de Pesos de Autoatenção")
    plt.xlabel("Tokens de Contexto (Keys)")
    plt.ylabel("Tokens Atuais (Queries)")
    plt.show()

plotar_heatmap_atencao(pesos)
